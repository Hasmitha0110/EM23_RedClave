{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1445db3a",
   "metadata": {},
   "source": [
    "# EM23_RedClave_Task1_HeartAttack.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f355421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Config - change these before running ===\n",
    "TEAM_CODE = \"TC01\"                # Replace with your TeamCode (e.g., TC01)\n",
    "TEAM_NAME = \"TeamRapidAid\"        # Replace with your TeamName (no spaces preferred)\n",
    "TRAIN_PATH = \"Heart_Attack_training_dataset.csv\"   # path to training CSV (upload to Colab)\n",
    "TEST_PATH = \"Hear_Attack_evaluation_dataset.csv\"   # path to test CSV (upload to Colab)\n",
    "\n",
    "OUTPUT_NOTEBOOK = f\"{TEAM_CODE}_{TEAM_NAME}_Task1_HeartAttack.ipynb\"\n",
    "METRICS_PNG = f\"{TEAM_CODE}_{TEAM_NAME}_Task1_Metrics.png\"\n",
    "PRED_CSV = f\"{TEAM_CODE}_{TEAM_NAME}_Task1_Predictions.csv\"\n",
    "\n",
    "print(\"Output files will be:\", METRICS_PNG, \"and\", PRED_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b6e9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install extras if running on a fresh Colab environment\n",
    "try:\n",
    "    import xgboost\n",
    "except Exception as e:\n",
    "    print('xgboost not found. Installing...')\n",
    "    !pip install -q xgboost\n",
    "    import xgboost\n",
    "print('Required libs available.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5195cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Libraries imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84717340",
   "metadata": {},
   "source": [
    "## 1) Load data and quick EDA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee28e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print('Train shape:', train.shape)\n",
    "print('Test shape:', test.shape)\n",
    "display(train.head())\n",
    "display(train.info())\n",
    "display(train.describe(include='all').T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ace16",
   "metadata": {},
   "source": [
    "## 2) Preprocessing plan\n",
    "\n",
    "- Parse `bp` (systolic/diastolic) into two numeric columns: `bp_sys`, `bp_dia`.\n",
    "- Handle missing values: numeric -> median, categorical -> most frequent.\n",
    "- Encode categorical features: Ordinal/One-hot as appropriate.\n",
    "- Scale numeric features using StandardScaler.\n",
    "\n",
    "We will create a preprocessing pipeline and then train models inside a pipeline to avoid leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2553bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and feature engineering\n",
    "def preprocess_dfs(df):\n",
    "    df = df.copy()\n",
    "    # parse bp column\n",
    "    if 'bp' in df.columns:\n",
    "        bp_split = df['bp'].astype(str).str.split('/', expand=True)\n",
    "        df['bp_sys'] = pd.to_numeric(bp_split[0], errors='coerce')\n",
    "        df['bp_dia'] = pd.to_numeric(bp_split[1], errors='coerce')\n",
    "        df.drop(columns=['bp'], inplace=True)\n",
    "    # ensure target exists in train\n",
    "    return df\n",
    "\n",
    "train = preprocess_dfs(train)\n",
    "test = preprocess_dfs(test)\n",
    "\n",
    "# Identify feature lists\n",
    "target = 'heart_attack_risk'\n",
    "id_col = 'patient_id'\n",
    "features = [c for c in train.columns if c not in [target, id_col]]\n",
    "\n",
    "# automatic dtype inference for categorical vs numeric\n",
    "num_cols = train[features].select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "cat_cols = [c for c in features if c not in num_cols]\n",
    "\n",
    "print('Numeric cols:', num_cols)\n",
    "print('Categorical cols:', cat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f82a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric pipeline \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_cols),\n",
    "        ('cat', categorical_transformer, cat_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d091c6",
   "metadata": {},
   "source": [
    "## 3) Train/Test split and model comparison\n",
    "\n",
    "We'll train three candidate models: Logistic Regression, Random Forest, and XGBoost. We will prioritize Recall (since competition scoring uses Recall). We'll use stratified split and report multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ace0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into train/validation (stratified)\n",
    "X = train[features].copy()\n",
    "y = train[target].copy()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)\n",
    "\n",
    "print('Shapes:', X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "\n",
    "# Build pipelines for models\n",
    "pipelines = {\n",
    "    'logreg': Pipeline(steps=[('pre', preprocessor), ('clf', LogisticRegression(max_iter=1000, class_weight='balanced'))]),\n",
    "    'rf': Pipeline(steps=[('pre', preprocessor), ('clf', RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42))]),\n",
    "    'xgb': Pipeline(steps=[('pre', preprocessor), ('clf', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))])\n",
    "}\n",
    "\n",
    "# Quick train and evaluate function\n",
    "def evaluate_model(pipeline, X_tr, y_tr, X_v, y_v, name='model'):\n",
    "    pipeline.fit(X_tr, y_tr)\n",
    "    preds = pipeline.predict(X_v)\n",
    "    probs = pipeline.predict_proba(X_v)[:,1] if hasattr(pipeline, 'predict_proba') else None\n",
    "    acc = accuracy_score(y_v, preds)\n",
    "    prec = precision_score(y_v, preds, zero_division=0)\n",
    "    rec = recall_score(y_v, preds)\n",
    "    f1 = f1_score(y_v, preds)\n",
    "    roc = roc_auc_score(y_v, probs) if probs is not None else np.nan\n",
    "    print(f\"=== {name} ===\\nAccuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}, ROC-AUC: {roc:.4f}\\n\")\n",
    "    return {'name': name, 'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'roc_auc': roc, 'pipeline': pipeline, 'preds': preds}\n",
    "\n",
    "results = []\n",
    "for name, pipe in pipelines.items():\n",
    "    print('Training', name)\n",
    "    res = evaluate_model(pipe, X_train, y_train, X_val, y_val, name=name)\n",
    "    results.append(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa1bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics DataFrame and save as PNG\n",
    "metrics_df = pd.DataFrame([{'Model':r['name'],'Accuracy':r['accuracy'],'Precision':r['precision'],'Recall':r['recall'],'F1':r['f1'],'ROC-AUC':r['roc_auc']} for r in results])\n",
    "metrics_df = metrics_df.sort_values('Recall', ascending=False).reset_index(drop=True)\n",
    "display(metrics_df.style.format({'Accuracy':'{:.4f}','Precision':'{:.4f}','Recall':'{:.4f}','F1':'{:.4f}','ROC-AUC':'{:.4f}'}))\n",
    "\n",
    "# Save a figure with the table for screenshot requirement\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(8,2 + 0.6*len(metrics_df)))\n",
    "ax.axis('off')\n",
    "tbl = ax.table(cellText=np.round(metrics_df[['Accuracy','Precision','Recall','F1','ROC-AUC']].values,4),\n",
    "               rowLabels=metrics_df['Model'].values,\n",
    "               colLabels=['Accuracy','Precision','Recall','F1','ROC-AUC'],\n",
    "               cellLoc='center', loc='center')\n",
    "tbl.auto_set_font_size(False)\n",
    "tbl.set_fontsize(10)\n",
    "plt.title('Model evaluation metrics (validation)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(METRICS_PNG, dpi=200)\n",
    "print('Saved metrics image to', METRICS_PNG)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e41edf",
   "metadata": {},
   "source": [
    "## 4) Hyperparameter tuning (optional but recommended)\n",
    "\n",
    "Because Recall is most important, we tune models using Recall as the scoring metric. We'll demonstrate GridSearchCV for RandomForest and XGBoost with stratified CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86602b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "rf_params = {'clf__n_estimators':[100,200], 'clf__max_depth':[None,10,20]}\n",
    "xgb_params = {'clf__n_estimators':[100,200], 'clf__max_depth':[3,6], 'clf__scale_pos_weight':[1,5]}\n",
    "\n",
    "best_pipelines = {}\n",
    "\n",
    "print('Tuning RandomForest...')\n",
    "gs_rf = GridSearchCV(pipelines['rf'], rf_params, scoring='recall', cv=cv, n_jobs=-1, verbose=1)\n",
    "gs_rf.fit(X_train, y_train)\n",
    "print('Best RF params:', gs_rf.best_params_, 'best recall:', gs_rf.best_score_)\n",
    "best_pipelines['rf'] = gs_rf.best_estimator_\n",
    "\n",
    "print('\\nTuning XGBoost...')\n",
    "gs_xgb = GridSearchCV(pipelines['xgb'], xgb_params, scoring='recall', cv=cv, n_jobs=-1, verbose=1)\n",
    "gs_xgb.fit(X_train, y_train)\n",
    "print('Best XGB params:', gs_xgb.best_params_, 'best recall:', gs_xgb.best_score_)\n",
    "best_pipelines['xgb'] = gs_xgb.best_estimator_\n",
    "\n",
    "# Evaluate best models on validation set\n",
    "tuned_results = []\n",
    "for name, pipe in best_pipelines.items():\n",
    "    tuned_results.append(evaluate_model(pipe, X_train, y_train, X_val, y_val, name='tuned_'+name))\n",
    "\n",
    "tuned_df = pd.DataFrame([{'Model':r['name'],'Accuracy':r['accuracy'],'Precision':r['precision'],'Recall':r['recall'],'F1':r['f1'],'ROC-AUC':r['roc_auc']} for r in tuned_results])\n",
    "display(tuned_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b614605a",
   "metadata": {},
   "source": [
    "## 5) Final model training\n",
    "\n",
    "Choose the best model (based on Recall on validation). Retrain it on the full training dataset and generate predictions for the test set. Then save predictions CSV with required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5c4ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model by recall from tuned + baseline results\n",
    "all_results = results + tuned_results if 'tuned_results' in globals() else results\n",
    "best = max(all_results, key=lambda x: x['recall'])\n",
    "print('Selected model for final predictions:', best['name'])\n",
    "\n",
    "# Retrain selected model on full training data\n",
    "final_pipeline = best['pipeline']\n",
    "final_pipeline.fit(X, y)  # train on full training set\n",
    "\n",
    "# Prepare test features (ensure same columns order)\n",
    "X_test = test[[c for c in test.columns if c != id_col]].copy()\n",
    "\n",
    "# Predict\n",
    "test_preds = final_pipeline.predict(X_test)\n",
    "test_preds = (test_preds > 0.5).astype(int) if test_preds.dtype==bool else test_preds\n",
    "\n",
    "# Build submission DataFrame\n",
    "submission = pd.DataFrame({id_col: test[id_col], 'heart_attack_risk': test_preds})\n",
    "submission.to_csv(PRED_CSV, index=False)\n",
    "print('Saved predictions to', PRED_CSV)\n",
    "display(submission.head())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
