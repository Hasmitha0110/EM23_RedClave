{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b04f9dbe",
   "metadata": {},
   "source": [
    "\n",
    "# Task 02 — House Price Prediction (Regression)\n",
    "\n",
    "**Files this notebook will produce (replace `TeamCode` and `TeamName` with your actual team code/name before submission):**\n",
    "\n",
    "- `TeamCode_TeamName_Task2_HousePrice.ipynb` — this notebook (you already have it).\n",
    "- `TeamCode_TeamName_Task2_Metrics.png` — screenshot image with the evaluation metrics (RMSE, MAE, R²).\n",
    "- `TeamCode_TeamName_Task2_Predictions.csv` — final submission CSV (**must** contain `house_id` and `predicted_price` columns).\n",
    "- `TeamCode_TeamName_Task2_FinalModel.joblib` — trained final model (optional to submit).\n",
    "\n",
    "**How to run (Google Colab recommended):**\n",
    "1. Open this notebook in Colab (`File > Upload notebook`) or upload `train.csv` and `test.csv` into the Colab session.  \n",
    "2. Run cells from top to bottom. The notebook contains safe default modeling, preprocessing, evaluation, and instructions.  \n",
    "3. After running, the predictions CSV and metrics PNG will be saved in the runtime working directory. Download and add them to your GitHub repo with the required file names above.\n",
    "\n",
    "> **Important:** Replace `TeamCode` and `TeamName` in filenames before uploading to the competition (or rename the produced files accordingly).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4476e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "\n",
    "print('Environment ready. Current working dir:', os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Load train.csv and test.csv ----\n",
    "# This block works in regular Jupyter and in Google Colab.\n",
    "def _read_with_date(path):\n",
    "    cols = pd.read_csv(path, nrows=0).columns.tolist()\n",
    "    if 'sale_date' in cols:\n",
    "        return pd.read_csv(path, parse_dates=['sale_date'])\n",
    "    else:\n",
    "        return pd.read_csv(path)\n",
    "\n",
    "train_path = 'train.csv'\n",
    "test_path = 'test.csv'\n",
    "\n",
    "if os.path.exists(train_path) and os.path.exists(test_path):\n",
    "    train = _read_with_date(train_path)\n",
    "    test = _read_with_date(test_path)\n",
    "else:\n",
    "    # Colab upload flow\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        print('Please upload train.csv and test.csv in the file chooser. You can upload both at once.')\n",
    "        uploaded = files.upload()\n",
    "        # uploaded is a dict of filename -> bytes\n",
    "        for fn in uploaded.keys():\n",
    "            print('Uploaded:', fn)\n",
    "        train = _read_with_date('train.csv')\n",
    "        test = _read_with_date('test.csv')\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError('train.csv and test.csv not found. Upload them to the working directory or use Colab files.upload().') from e\n",
    "\n",
    "print('Train shape:', train.shape)\n",
    "print('Test shape:', test.shape)\n",
    "train.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031c7ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick overview\n",
    "display(train.describe(include='all').T)\n",
    "print('\\nMissing values in train:')\n",
    "print(train.isna().sum().sort_values(ascending=False).head(20))\n",
    "print('\\nColumns:', train.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe39fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Safe and robust feature engineering function\n",
    "def feature_engineer(df, zip_count_map=None, is_train=True):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure numeric columns are numeric\n",
    "    for col in ['built_year', 'renovated_year', 'living_area', 'lot_area', \n",
    "                'above_area', 'basement_area', 'num_bedrooms', 'num_bathrooms', \n",
    "                'num_floors', 'view_rating', 'condition_index', 'construction_grade',\n",
    "                'neighbor_living_area', 'neighbor_lot_area']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Parse sale_date safely\n",
    "    if 'sale_date' in df.columns:\n",
    "        df['sale_year'] = pd.to_datetime(df['sale_date'], errors='coerce').dt.year\n",
    "    else:\n",
    "        df['sale_year'] = pd.Timestamp.now().year  # fallback\n",
    "\n",
    "    # Fill missing years with median to avoid NaN issues\n",
    "    if 'built_year' in df.columns:\n",
    "        df['built_year'] = df['built_year'].fillna(df['built_year'].median())\n",
    "\n",
    "    # Age feature\n",
    "    df['age'] = df['sale_year'] - df['built_year']\n",
    "\n",
    "    # Renovation features\n",
    "    if 'renovated_year' in df.columns:\n",
    "        df['renovated_year'] = df['renovated_year'].fillna(0)\n",
    "        df.loc[df['renovated_year'] < 0, 'renovated_year'] = 0\n",
    "        df['years_since_renov'] = df['sale_year'] - df['renovated_year']\n",
    "        df.loc[df['renovated_year'] <= 0, 'years_since_renov'] = 0\n",
    "        df['was_renovated'] = (df['renovated_year'] > 0).astype(int)\n",
    "    else:\n",
    "        df['years_since_renov'] = 0\n",
    "        df['was_renovated'] = 0\n",
    "\n",
    "    # Area-related features\n",
    "    df['above_area'] = df['above_area'].fillna(0)\n",
    "    df['basement_area'] = df['basement_area'].fillna(0)\n",
    "    df['total_area'] = df['above_area'] + df['basement_area']\n",
    "    df['has_basement'] = (df['basement_area'] > 0).astype(int)\n",
    "\n",
    "    # Ratio feature\n",
    "    df['lot_living_ratio'] = df['lot_area'] / (df['living_area'].replace(0, np.nan))\n",
    "    df['lot_living_ratio'] = df['lot_living_ratio'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    # Zip frequency encoding\n",
    "    if 'zip_area' in df.columns:\n",
    "        if is_train:\n",
    "            zip_count_map = df['zip_area'].value_counts().to_dict()\n",
    "        df['zip_freq'] = df['zip_area'].map(zip_count_map).fillna(0)\n",
    "    else:\n",
    "        df['zip_freq'] = 0\n",
    "\n",
    "    return df, zip_count_map\n",
    "\n",
    "\n",
    "# ✅ Re-run these lines\n",
    "train_fe, zip_map = feature_engineer(train, is_train=True)\n",
    "test_fe, _ = feature_engineer(test, zip_map, is_train=False)\n",
    "\n",
    "print('✅ Feature engineering complete. Example:')\n",
    "display(train_fe[['sale_date','sale_year','age','years_since_renov','was_renovated','total_area']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db2f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare feature list\n",
    "target_col = 'target_price'\n",
    "id_col = 'house_id'\n",
    "\n",
    "# Drop columns that won't be used as features (but keep id_col for test)\n",
    "drop_cols = ['sale_date'] if 'sale_date' in train_fe.columns else []\n",
    "\n",
    "# Select candidate numerical features\n",
    "numeric_feats = [\n",
    "    'num_bedrooms','num_bathrooms','living_area','lot_area','num_floors',\n",
    "    'is_waterfront','view_rating','condition_index','construction_grade',\n",
    "    'above_area','basement_area','built_year','renovated_year',\n",
    "    'neighbor_living_area','neighbor_lot_area',\n",
    "    # engineered\n",
    "    'sale_year','age','years_since_renov','total_area','has_basement','lot_living_ratio','zip_freq'\n",
    "]\n",
    "\n",
    "# Keep only columns that exist\n",
    "numeric_feats = [c for c in numeric_feats if c in train_fe.columns]\n",
    "\n",
    "# For categorical, we'll treat 'zip_area' cautiously; if it has small uniques, one-hot it.\n",
    "cat_feats = []\n",
    "if 'zip_area' in train_fe.columns:\n",
    "    if train_fe['zip_area'].nunique() <= 20:\n",
    "        cat_feats.append('zip_area')  # will one-hot encode\n",
    "    else:\n",
    "        # we already encoded zip_freq; skip one-hot to avoid huge dim\n",
    "        pass\n",
    "\n",
    "print('Numeric features used:', numeric_feats)\n",
    "print('Categorical features used:', cat_feats)\n",
    "\n",
    "# Train/validation split\n",
    "X = train_fe[numeric_feats + cat_feats].copy()\n",
    "y = train_fe[target_col].copy()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "print('X_train shape:', X_train.shape, 'X_val shape:', X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38595595",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "if len(cat_feats) > 0:\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer, numeric_feats),\n",
    "        ('cat', categorical_transformer, cat_feats)\n",
    "    ])\n",
    "else:\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer, numeric_feats)\n",
    "    ])\n",
    "\n",
    "# Example: a pipeline with RandomForest\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preproc', preprocessor),\n",
    "    ('model', RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Quick baseline: Linear Regression pipeline\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('preproc', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb1b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred): \n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Train linear regression baseline\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "y_pred_lr = lr_pipeline.predict(X_val)\n",
    "print('Linear Regression  - RMSE: {:.2f}, MAE: {:.2f}, R2: {:.4f}'.format(\n",
    "    rmse(y_val, y_pred_lr), \n",
    "    mean_absolute_error(y_val, y_pred_lr), \n",
    "    r2_score(y_val, y_pred_lr)\n",
    "))\n",
    "\n",
    "# Train random forest baseline\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "y_pred_rf = rf_pipeline.predict(X_val)\n",
    "print('Random Forest      - RMSE: {:.2f}, MAE: {:.2f}, R2: {:.4f}'.format(\n",
    "    rmse(y_val, y_pred_rf), \n",
    "    mean_absolute_error(y_val, y_pred_rf), \n",
    "    r2_score(y_val, y_pred_rf)\n",
    "))\n",
    "\n",
    "# Train a Gradient Boosting baseline (sklearn)\n",
    "gbr = Pipeline(steps=[('preproc', preprocessor),\n",
    "                      ('model', GradientBoostingRegressor(n_estimators=300, random_state=42))])\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred_gbr = gbr.predict(X_val)\n",
    "print('GradientBoosting   - RMSE: {:.2f}, MAE: {:.2f}, R2: {:.4f}'.format(\n",
    "    rmse(y_val, y_pred_gbr), \n",
    "    mean_absolute_error(y_val, y_pred_gbr), \n",
    "    r2_score(y_val, y_pred_gbr)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a7c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Much smaller and faster search\n",
    "from scipy.stats import randint\n",
    "param_dist = {\n",
    "    'model__n_estimators': [50, 100],  # Reduced options\n",
    "    'model__max_depth': [10, 15],      # Reduced options  \n",
    "    'model__min_samples_split': [5, 10] # Reduced options\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    rf_pipeline, \n",
    "    param_dist, \n",
    "    n_iter=4,  # Only 4 iterations\n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    cv=2,      # Only 2-fold CV\n",
    "    random_state=42, \n",
    "    n_jobs=1,  # Use only 1 job to avoid overhead\n",
    "    verbose=1\n",
    ")\n",
    "rs.fit(X_train, y_train)\n",
    "print('Best RF params:', rs.best_params_)\n",
    "best_rf = rs.best_estimator_\n",
    "y_pred_best_rf = best_rf.predict(X_val)\n",
    "print('Tuned Random Forest - RMSE: {:.2f}, MAE: {:.2f}, R2: {:.4f}'.format(\n",
    "    rmse(y_val, y_pred_best_rf), \n",
    "    mean_absolute_error(y_val, y_pred_best_rf), \n",
    "    r2_score(y_val, y_pred_best_rf)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f9d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose final model: pick the best among the trained models automatically\n",
    "candidates = {\n",
    "    'LinearRegression': (lr_pipeline, rmse(y_val, y_pred_lr)),\n",
    "    'RandomForest': (rf_pipeline, rmse(y_val, y_pred_rf)),\n",
    "    'GradientBoosting': (gbr, rmse(y_val, y_pred_gbr)),\n",
    "    'TunedRandomForest': (best_rf if 'best_rf' in globals() else rf_pipeline, rmse(y_val, y_pred_best_rf) if 'y_pred_best_rf' in globals() else rmse(y_val, y_pred_rf))\n",
    "}\n",
    "\n",
    "best_name = min(candidates.items(), key=lambda kv: kv[1][1])[0]\n",
    "best_pipeline = candidates[best_name][0]\n",
    "print('Selected final model:', best_name)\n",
    "\n",
    "# Refit final model on full training data (train_fe)\n",
    "X_full = train_fe[numeric_feats + cat_feats].copy()\n",
    "y_full = train_fe[target_col].copy()\n",
    "best_pipeline.fit(X_full, y_full)\n",
    "\n",
    "# Prepare test features (ensure same feature columns/order)\n",
    "X_test = test_fe[numeric_feats + cat_feats].copy()\n",
    "preds = best_pipeline.predict(X_test)\n",
    "\n",
    "# Prepare predictions DataFrame\n",
    "preds_df = pd.DataFrame({\n",
    "    id_col: test_fe[id_col],\n",
    "    'predicted_price': np.round(preds).astype(int)\n",
    "})\n",
    "\n",
    "# Save the predictions CSV with required filename (update TeamCode and TeamName below)\n",
    "teamcode = 'TeamCode'\n",
    "teamname = 'TeamName'\n",
    "pred_filename = f'{teamcode}_{teamname}_Task2_Predictions.csv'\n",
    "preds_df.to_csv(pred_filename, index=False)\n",
    "print('Predictions saved to', pred_filename)\n",
    "\n",
    "# Save the final trained model\n",
    "model_filename = f'{teamcode}_{teamname}_Task2_FinalModel.joblib'\n",
    "joblib.dump(best_pipeline, model_filename)\n",
    "print('Trained model saved to', model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88a28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute evaluation metrics on validation for the selected model (if available)\n",
    "if 'y_val' in globals():\n",
    "    if best_name == 'LinearRegression':\n",
    "        y_val_pred = y_pred_lr\n",
    "    elif best_name == 'RandomForest':\n",
    "        y_val_pred = y_pred_rf\n",
    "    elif best_name == 'GradientBoosting':\n",
    "        y_val_pred = y_pred_gbr\n",
    "    else:\n",
    "        y_val_pred = y_pred_best_rf if 'y_pred_best_rf' in globals() else y_pred_rf\n",
    "\n",
    "    # Use our custom rmse function instead of mean_squared_error with squared=False\n",
    "    final_rmse = rmse(y_val, y_val_pred)\n",
    "    final_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    final_r2 = r2_score(y_val, y_val_pred)\n",
    "else:\n",
    "    final_rmse = final_mae = final_r2 = None\n",
    "\n",
    "# Save a PNG screenshot with the metrics for submission\n",
    "metrics_filename = f'{teamcode}_{teamname}_Task2_Metrics.png'\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.axis('off')\n",
    "txt = f\"\"\"Validation metrics ({best_name})\n",
    "RMSE: {final_rmse:.2f}\n",
    "MAE: {final_mae:.2f}\n",
    "R²: {final_r2:.4f}\n",
    "\"\"\" if final_rmse is not None else 'Metrics not available'\n",
    "plt.text(0.01, 0.5, txt, fontsize=12, va='center')\n",
    "plt.savefig(metrics_filename, bbox_inches='tight', dpi=150)\n",
    "plt.close()\n",
    "print('Metrics screenshot saved to', metrics_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2b0e50",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Final notes & checklist before submission\n",
    "\n",
    "1. **Rename files**: Replace `TeamCode` and `TeamName` in the produced filenames with your official team code and team name exactly as required by the competition. Example final names:\n",
    "   - `T01_CoolTeam_Task2_HousePrice.ipynb`\n",
    "   - `T01_CoolTeam_Task2_Metrics.png`\n",
    "   - `T01_CoolTeam_Task2_Predictions.csv`\n",
    "\n",
    "2. **Verify CSV format**: Open the saved CSV and confirm it has exactly two columns: `house_id`, `predicted_price`. No index column should be included.\n",
    "\n",
    "3. **Screenshot requirement**: The PNG `*_Metrics.png` is required as proof of training metrics; make sure it is visible in your repo.\n",
    "\n",
    "4. **If you want to improve performance**:\n",
    "   - Try feature selection, target transformation (log), stacking, stronger hyperparameter tuning, or use LightGBM/XGBoost (available in Colab).\n",
    "   - Use K-Fold CV and out-of-fold predictions for more robust model selection.\n",
    "\n",
    "5. **Troubleshooting**:\n",
    "   - If any column names differ from the assumed ones above, edit the feature lists in the notebook accordingly.\n",
    "   - If zip codes are many, the notebook uses `zip_freq` (frequency encoding) to avoid huge one-hot expansions.\n",
    "---\n",
    "\n",
    "Good luck in the competition — run this in Colab, upload the resulting CSV + PNG + notebook to your GitHub repository, and you should be ready for submission!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
